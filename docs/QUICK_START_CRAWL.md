# 爬虫数据导入系统 - 快速上手

## 🎯 目标
将小红书爬取的数据导入到 MySQL 数据库，供 AI RAG 系统检索使用。

---

## 📁 项目文件说明

### 核心文件

| 文件 | 功能 | 使用场景 |
|------|------|----------|
| `import_crawled_data.py` | 🆕 智能导入工具 | **推荐使用**，支持交互式和批量模式 |
| `crawl_import.py` | 旧版导入脚本 | 简单直接，适合快速测试 |
| `src/crawler.py` | 爬虫核心逻辑 | 封装了小红书API调用 |
| `CRAWL_IMPORT_GUIDE.md` | 详细使用指南 | 完整的操作说明 |
| `run_import.bat` | Windows快捷脚本 | 双击运行 |

---

## 🚀 三种使用方式

### 方式1️⃣：交互式导入（最简单）

```bash
uv run import_crawled_data.py
```

**步骤：**
1. 输入关键词（如"学一食堂"）
2. 确认景点绑定
3. 选择爬取数量
4. 预览并确认导入

**优点：** 一步步引导，不会出错

---

### 方式2️⃣：批量导入（适合多个关键词）

```bash
uv run import_crawled_data.py "学一食堂" "图书馆" "运动场" "北邮星塔"
```

**优点：** 一次性处理多个关键词，无需人工干预

---

### 方式3️⃣：Windows 用户（双击运行）

直接双击 `run_import.bat` 文件

---

## 📊 数据流程图

```
小红书网站
    ↓ (爬取)
原始JSON数据
    ↓ (清洗)
格式化数据
    ↓ (智能匹配)
绑定景点ID
    ↓ (保存)
MySQL数据库
    ↓ (检索)
AI RAG系统
```

---

## 💡 实际使用案例

### 案例1：导入食堂评价

```bash
# 1. 启动工具
uv run import_crawled_data.py

# 2. 输入信息
关键词: 学一食堂
数量: 5

# 3. 结果
✅ 智能匹配到景点: 学生食堂 (ID: 44)
✅ 成功导入 5 条日记

# 4. 测试AI
问题: "学一食堂好吃吗？"
AI回答: 根据同学们的反馈，学一食堂的...（从导入的数据中检索）
```

---

### 案例2：批量导入多个景点

```bash
# 一次性导入4个景点的数据
uv run import_crawled_data.py "学一食堂" "图书馆自习" "运动场跑步" "北邮星塔拍照"

# 系统自动：
# - 学一食堂 → 绑定到"学生食堂" (ID: 44)
# - 图书馆自习 → 绑定到"图书馆" (ID: 57)
# - 运动场跑步 → 绑定到"运动场" (ID: 46)
# - 北邮星塔拍照 → 绑定到"北邮星塔" (ID: 54)

# 结果：约 20 条日记导入完成
```

---

## 🔧 配置说明

### 必需配置：Cookie

在 `.env` 文件中配置小红书 Cookie：

```env
XHS_COOKIE=你的完整cookie字符串
```

**如何获取：**
1. 浏览器登录小红书
2. F12 → Network → 复制任意请求的 Cookie
3. 粘贴到 .env 文件

**有效期：** 通常 7-30 天

---

### 可选配置：数据库

`.env` 文件中已有的配置：

```env
DATABASE_URL=mysql+pymysql://root:root@127.0.0.1:3306/campus_nav
```

默认即可，无需修改。

---

## 📈 数据库结构

### 导入前

```sql
-- user 表
| id | username    |
|----|-------------|
| 1  | student_A   |
| 2  | foodie_B    |
| 3  | coder_C     |
```

### 导入后

```sql
-- 新增爬虫账号
| id | username    |
|----|-------------|
| 5  | spider_bot  | ← 🤖 爬虫专用账号

-- diary 表新增
| id  | user_id | spot_id | title              | view_count |
|-----|---------|---------|--------------------|-----------:|
| 232 | 5       | 44      | [搬运] 学一食堂探店   | 1234       |
| 233 | 5       | 44      | [搬运] 北邮美食攻略   | 856        |
| ... | 5       | ...     | ...                | ...        |
```

---

## 🎨 智能景点匹配

系统使用模糊匹配算法自动识别景点：

| 输入关键词 | 匹配结果 | 相似度 | 景点ID |
|------------|----------|--------|--------|
| "学一食堂" | 学生食堂 | 85% | 44 |
| "学生餐厅" | 学生食堂 | 90% | 44 |
| "图书馆自习室" | 图书馆 | 75% | 57 |
| "操场跑步" | 运动场 | 70% | 46 |

**算法：** Levenshtein Distance（编辑距离）

---

## ✅ 验证导入结果

### 1. 查看数据库

```bash
uv run view_database.py
```

### 2. 通过 SQL 查询

```sql
-- 查看爬虫导入的所有日记
SELECT id, title, spot_id, view_count 
FROM diary 
WHERE user_id = 5;

-- 查看某个景点的日记
SELECT * FROM diary WHERE spot_id = 44;
```

### 3. 测试 API

```bash
# 启动后端
uv run uvicorn src.api:app --reload

# 访问
http://localhost:8000/diaries/spot/44
```

### 4. 测试 AI RAG

前端问 AI：
- "学一食堂好吃吗？"
- "图书馆自习怎么样？"
- "北邮哪个食堂最受欢迎？"

AI 会从导入的数据中检索并回答。

---

## ⚠️ 常见问题

### Q: Cookie 过期了？

**症状：** 返回空数据或 401 错误

**解决：** 重新获取 Cookie，更新 `.env`

---

### Q: 没有爬到数据？

**检查清单：**
- [ ] Cookie 是否配置
- [ ] 网络是否正常
- [ ] 关键词是否太冷门
- [ ] 查看终端错误信息

---

### Q: 如何避免重复导入？

系统会自动去重（通过标题判断）：
- 相同标题 → 跳过
- 不同标题 → 导入

建议：每次使用不同的关键词。

---

### Q: 图片能显示吗？

导入的是小红书的图片 URL，可能有以下问题：
- 防盗链：可能无法直接访问
- 过期：URL 可能失效

**解决方案：**
1. 开启爬虫的图片下载功能
2. 保存到本地 `downloads/media/`
3. 修改数据库中的 URL 为本地路径

---

## 🎯 完整演示

```bash
# Step 1: 检查环境
cd D:\Code\Tourism_system
uv run view_database.py

# Step 2: 导入数据
uv run import_crawled_data.py

# 按提示输入：
# 关键词: 学一食堂
# 数量: 5
# 确认: y

# Step 3: 验证
uv run view_database.py
# 应该看到新增 5 条日记

# Step 4: 测试 AI
uv run uvicorn src.api:app --reload
# 在前端问：学一食堂好吃吗？
```

---

## 📚 扩展阅读

- [完整使用指南](CRAWL_IMPORT_GUIDE.md)
- [项目README](README.md)
- [爬虫模块文档](src/tools/Spider_XHS/README.md)

---

## 💬 需要帮助？

1. 查看详细指南：`CRAWL_IMPORT_GUIDE.md`
2. 检查日志输出
3. 查看源码注释

---

**祝使用愉快！🎉**
